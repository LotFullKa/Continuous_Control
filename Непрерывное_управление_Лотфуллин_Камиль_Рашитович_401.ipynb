{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T22:46:50.460493Z",
     "start_time": "2024-05-04T22:46:50.452158Z"
    }
   },
   "source": [
    "# Управление с непрерывными действиями (Continuous Control) (<span style=\"color: green\">10 баллов за основную часть + 5 баллов за бонусную часть</span>)\n",
    "\n",
    "#### Дедлайн (жёсткий) задания: 04.05.2025,  UTC+3.\n",
    "\n",
    "#### При сдаче задания нужно  поместить в архив данный файл и папки с логами и видео, сохраняя относительные пути, и послать архив в систему сдачи.\n",
    "\n",
    "### <span style=\"color: red\"> Если работа была списана и/или сделана LLM, то за работу ставится 0 баллов. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Лотфуллин Камиль Рашитович, М05-401."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании предлагается решить задачу управления с непрерывными действиями, используя алгоритмы:\n",
    "\n",
    "- **Twin Delayed DDPG (TD3)** (**Раздел 6.1.6 (Алгоритм 23)**)\n",
    "- **Soft Actor-Critic (SAC)** (**Раздел 6.2.4 (Алгоритм 24)**)\n",
    "\n",
    "Оба алгоритма являются off-policy и считаются одними из наиболее эффективных для задач управления в непрерывном пространстве действий. Они основаны на базовом алгоритме **Deep Deterministic Policy Gradient (DDPG)** (**Раздел 6.1.5 (Алгоритм 22)**), который можно представить как \"DQN с отдельной нейросетью для аппроксимации жадной политики\". Основные отличия заключаются в различных стабилизационных приёмах:\n",
    "\n",
    "- TD3 обучает детерминированную политику, тогда как SAC использует стохастическую политику. Это означает, что в SAC достаточно просто сэмплировать действия из политики для исследования, в то время как в TD3 необходимо вручную добавлять шум к действиям.\n",
    "- TD3 добавляет к действиям обрезанный шум (clipped noise) при расчёте целевых значений, что помогает бороться с переоценкой. SAC использует формализм максимальной энтропии и добавляет бонус за энтропию в функцию ценности, поощряя более разнообразные действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.167674Z",
     "start_time": "2025-04-24T08:13:43.161443Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:09.769065Z",
     "iopub.status.busy": "2025-05-11T08:53:09.768497Z",
     "iopub.status.idle": "2025-05-11T08:53:14.255226Z",
     "shell.execute_reply": "2025-05-11T08:53:14.254417Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from logger import TensorboardSummaries as Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.374789Z",
     "start_time": "2025-04-24T08:13:43.369879Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:14.257994Z",
     "iopub.status.busy": "2025-05-11T08:53:14.257625Z",
     "iopub.status.idle": "2025-05-11T08:53:14.578713Z",
     "shell.execute_reply": "2025-05-11T08:53:14.578103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Среда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы создадим экземпляр среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.931381Z",
     "start_time": "2025-04-24T08:13:43.916456Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:14.624698Z",
     "iopub.status.busy": "2025-05-11T08:53:14.624137Z",
     "iopub.status.idle": "2025-05-11T08:53:14.887249Z",
     "shell.execute_reply": "2025-05-11T08:53:14.886493Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность вектора состояний dim = 24\n",
      "n_actions = 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"Размерность вектора состояний dim =\", state_dim)\n",
    "print(\"n_actions =\", action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на среду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.371255Z",
     "start_time": "2025-04-24T08:13:44.271132Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:14.889738Z",
     "iopub.status.busy": "2025-05-11T08:53:14.889492Z",
     "iopub.status.idle": "2025-05-11T08:53:15.083410Z",
     "shell.execute_reply": "2025-05-11T08:53:15.082699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на случайную политику и посмотрим, как это выглядит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.637836Z",
     "start_time": "2025-04-24T08:13:44.636248Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:15.086261Z",
     "iopub.status.busy": "2025-05-11T08:53:15.085744Z",
     "iopub.status.idle": "2025-05-11T08:53:15.089758Z",
     "shell.execute_reply": "2025-05-11T08:53:15.089070Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomActor():\n",
    "    def get_action(self, states):\n",
    "        assert len(states.shape) == 1, \"Не работает с батчами\"\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.835552Z",
     "start_time": "2025-04-24T08:13:44.806511Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:15.092234Z",
     "iopub.status.busy": "2025-05-11T08:53:15.091988Z",
     "iopub.status.idle": "2025-05-11T08:53:18.371229Z",
     "shell.execute_reply": "2025-05-11T08:53:18.370654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  1599\n",
      "done:  1684\n",
      "done:  1748\n",
      "done:  1850\n",
      "done:  3450\n",
      "done:  3571\n",
      "done:  5171\n",
      "done:  5245\n",
      "done:  5317\n",
      "done:  5407\n",
      "done:  5466\n",
      "done:  5520\n",
      "done:  5620\n",
      "done:  7220\n",
      "done:  8820\n"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "rewards_per_step = []\n",
    "actor = RandomActor()\n",
    "\n",
    "for i in range(10000):\n",
    "    a = actor.get_action(s)\n",
    "    s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "    rewards_per_step.append(r)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        print(\"done: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основном, каждый эпизод длится **1600 шагов** &mdash; это ограничение по времени, после которого происходит завершение эпизода. Однако иногда эпизод завершается раньше, если симуляция \"понимает\", что агент, например, **упал** или **разбился** &mdash; то есть ситуация явно неудачная для продолжения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что мы получаем при использовании случайной политики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.839119Z",
     "start_time": "2025-04-24T08:13:45.836680Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:18.373853Z",
     "iopub.status.busy": "2025-05-11T08:53:18.373612Z",
     "iopub.status.idle": "2025-05-11T08:53:18.379010Z",
     "shell.execute_reply": "2025-05-11T08:53:18.378155Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, n_games=1, t_max=1600):\n",
    "    '''\n",
    "    Запускает n_games эпизодов и возвращает массив наград.\n",
    "\n",
    "    Возвращает\n",
    "    -------\n",
    "    rewards: np.array\n",
    "        Массив наград\n",
    "    '''\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        R = 0\n",
    "\n",
    "        for _ in range(t_max):\n",
    "\n",
    "            ### Ваш код\n",
    "            action = actor.get_action(s)\n",
    "            ###\n",
    "\n",
    "            assert (action.max() <= 1).all() and  (action.min() >= -1).all()\n",
    "\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.180382Z",
     "start_time": "2025-04-24T08:13:45.839786Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:18.381418Z",
     "iopub.status.busy": "2025-05-11T08:53:18.380982Z",
     "iopub.status.idle": "2025-05-11T08:53:22.342103Z",
     "shell.execute_reply": "2025-05-11T08:53:22.341508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/videos_test folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_test\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.184346Z",
     "start_time": "2025-04-24T08:13:47.181920Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.344684Z",
     "iopub.status.busy": "2025-05-11T08:53:22.344248Z",
     "iopub.status.idle": "2025-05-11T08:53:22.349872Z",
     "shell.execute_reply": "2025-05-11T08:53:22.349286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_test/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_test').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Буфер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, что и в DQN. Вы можете просто скопировать код из вашего задания по DQN.\n",
    "\n",
    "#### Напомним интерфейс:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)`  &mdash; сохраняет кортеж (s,a,r,s',done) в буфер.\n",
    "* `exp_replay.sample(batch_size)` &mdash; возвращает наблюдения, действия, награды, следующие наблюдения и is_done для `batch_size` случайных сэмплов.\n",
    "* `len(exp_replay)` &mdash; возвращает количество элементов, хранящихся в буфере на данный момент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.196890Z",
     "start_time": "2025-04-24T08:13:47.184939Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.352165Z",
     "iopub.status.busy": "2025-05-11T08:53:22.351739Z",
     "iopub.status.idle": "2025-05-11T08:53:22.358528Z",
     "shell.execute_reply": "2025-05-11T08:53:22.357852Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Создаёт буфер реплеев.\n",
    "        \"\"\"\n",
    "        self._storage = deque(maxlen=size)\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add_random(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Убедитесь, что _storage не превзойдёт по размерам _maxsize.\n",
    "        Убедитесь, что FIFO правило выполняется: старейшие прецеденты должны удаляться раньше всех.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        self._storage.append(data)\n",
    "\n",
    "    def sample_random(self, batch_size):\n",
    "        \"\"\"\n",
    "        Сэмплирование батча переходов.\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            батч наблюдений (состояний)\n",
    "        act_batch: np.array\n",
    "            батч действий, выполненных на основе obs_batch\n",
    "        rew_batch: np.array\n",
    "            награды, полученные в качестве результата выполнения act_batch\n",
    "        next_obs_batch: np.array\n",
    "            следующие наблюдения (состояния), полученные после выполнения act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1, если выполнение act_batch[i] повлекло\n",
    "            окончание эпизода и 0 иначе.\n",
    "        \"\"\"\n",
    "\n",
    "        #< случайно сгенерировать batch_size индексов сэмплов в буфере >\n",
    "        indices = np.random.choice(len(self._storage), batch_size, replace=False)\n",
    "\n",
    "        # собрать <s,a,r,s',done> для каждого индекса\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for idx in indices:\n",
    "            sample = self._storage[idx]\n",
    "            states.append(sample[0])\n",
    "            actions.append(sample[1])\n",
    "            rewards.append(sample[2])\n",
    "            next_states.append(sample[3])\n",
    "            dones.append(sample[4])\n",
    "        # < states > , < actions >, < rewards >,  < next_states >, < is_done >\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.271903Z",
     "start_time": "2025-04-24T08:13:47.197662Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.360866Z",
     "iopub.status.busy": "2025-05-11T08:53:22.360635Z",
     "iopub.status.idle": "2025-05-11T08:53:22.645738Z",
     "shell.execute_reply": "2025-05-11T08:53:22.644917Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add_random(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"Размер буфера должен быть равен 10, потому что это максимальная вместимость\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для записи в буфер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.274946Z",
     "start_time": "2025-04-24T08:13:47.272633Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.648517Z",
     "iopub.status.busy": "2025-05-11T08:53:22.648100Z",
     "iopub.status.idle": "2025-05-11T08:53:22.652834Z",
     "shell.execute_reply": "2025-05-11T08:53:22.652204Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Играется в среде ровно n шагов и записывается каждая пятёрка (s, a, r, s', done) в буффер.\n",
    "\n",
    "    Возвращает\n",
    "    -------\n",
    "        sum_rewards: float\n",
    "            суммарную награду за n шагов.\n",
    "        s: float\n",
    "            состояние, в котором осталась среда.\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(n_steps):\n",
    "\n",
    "        ### Ваш код\n",
    "        a = agent.get_action(s)\n",
    "        ###\n",
    "\n",
    "        ns, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        exp_replay.add_random(s, a, r, ns, terminated)\n",
    "\n",
    "        s = env.reset()[0] if terminated or truncated else ns\n",
    "\n",
    "        sum_rewards += r\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:54:30.667197Z",
     "start_time": "2025-04-23T20:54:30.663277Z"
    }
   },
   "source": [
    "# Критик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте начнём с модели критика &mdash; она одинаковая для **TD3** и **SAC**. С одной стороны, он будет приближать оптимальную функцию $Q^*(s, a)$, а с другой &mdash; оценивать текущего актора $\\pi$, то есть рассматриваться как \n",
    "$Q^{\\pi}(s, a)$. Этот критик принимает на вход как состояние $s$, так и действие $a,$ а на выходе выдаёт скалярное значение.\n",
    "\n",
    "Важно: если модель возвращает скаляр на выходе, хорошей практикой является применение .squeeze(), чтобы избежать неожиданного broadcast-а, поскольку тензор формы [batch_size, 1] может автоматически расширяться при операциях с другими тензорами.\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейронная сеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(1 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.907009Z",
     "start_time": "2025-04-24T08:13:47.895580Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.655343Z",
     "iopub.status.busy": "2025-05-11T08:53:22.654790Z",
     "iopub.status.idle": "2025-05-11T08:53:22.660171Z",
     "shell.execute_reply": "2025-05-11T08:53:22.659566Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Ваш код\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        ###\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "\n",
    "        ### Ваш код\n",
    "        x = torch.cat([states, actions], dim=-1)\n",
    "        output = self.net(x)\n",
    "        ###\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_qvalues(self, states, actions):\n",
    "        '''\n",
    "        Возвращает qvalues\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        qvalues: torch.tensor [batch_size]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        qvalues = self.forward(states, actions).squeeze(-1)\n",
    "        ###\n",
    "\n",
    "        assert len(qvalues.shape) == 1 and qvalues.shape[0] == states.shape[0]\n",
    "\n",
    "        return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:48.310526Z",
     "start_time": "2025-04-24T08:13:48.301042Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.662444Z",
     "iopub.status.busy": "2025-05-11T08:53:22.661898Z",
     "iopub.status.idle": "2025-05-11T08:53:22.681716Z",
     "shell.execute_reply": "2025-05-11T08:53:22.680981Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"TD3\")\n",
    "\n",
    "eval_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим политику, или актора $\\pi$. Необходимо смоделировать детерминированную политику. То есть, модель должна возвращать `action_dim` чисел в диапазоне $[-1, 1]$. К сожалению, детерминированные политики могут вызывать проблемы со стабильностью и исследованием, поэтому нам потребуется реализовать три \"режима\" работы этой политики:\n",
    "\n",
    "1. Первый режим &mdash; **жадный** &mdash; это просто прямой проход через сеть. Он будет использоваться для обучения актора.\n",
    "2. Второй режим &mdash; **режим исследования** &mdash; когда нужно добавить шум (например, гауссовский), чтобы собирать более разнообразные данные.\n",
    "3. Третий режим &mdash; **\"обрезанный шум\"** &mdash; используется при расчёте целевого значения для критика. Здесь мы хотим добавить шум к выходу актора, но не слишком сильный, поэтому используем обрезанный шум:\n",
    "$$\\pi_{\\theta}(s) + \\varepsilon, \\quad \\varepsilon = \\operatorname{clip}(\\epsilon, -0.5, 0.5), \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейросеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:49.201849Z",
     "start_time": "2025-04-24T08:13:49.191425Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.696174Z",
     "iopub.status.busy": "2025-05-11T08:53:22.692706Z",
     "iopub.status.idle": "2025-05-11T08:53:22.716721Z",
     "shell.execute_reply": "2025-05-11T08:53:22.715996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TD3_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        ### Ваш код\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ###\n",
    "\n",
    "    def forward(self, state):\n",
    "        ### Ваш код\n",
    "        output = self.net(state)\n",
    "        ###\n",
    "        return output\n",
    "\n",
    "    def get_best_action(self, states):\n",
    "        '''\n",
    "        Используется для оптимизации актора.\n",
    "        Требуется, чтобы действия были дифференцируемыми по параметрам модели.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        actions = self.forward(states)\n",
    "        ###\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, states, std_noise=0.1):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "        Поэтому к действиям необходимо добавлять шум.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "        std_noise: float\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            states = torch.tensor(states, device=self.device)\n",
    "            # dist = Normal(0, std_noise)\n",
    "            actions = self.forward(states)\n",
    "            actions += torch.normal(0, std_noise, size=actions.shape, device=self.device)\n",
    "            actions = torch.clamp(actions, -1., 1.)\n",
    "\n",
    "            actions = actions.cpu().numpy()\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def get_target_action(self, states, std_noise=0.2, clip_eta=0.5):\n",
    "        '''\n",
    "        Используется для генерации целевых значений при обучении критика.\n",
    "        Возвращает действия с добавленным \"обрезанным шумом\".\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        std_noise: float\n",
    "        clip_eta: float\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут также не нужны, используется только для генерации целевых значений\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            actions = self.forward(states)\n",
    "            raw_noise = Normal(0, std_noise).sample(actions.shape).to(self.device)\n",
    "            noise = torch.clamp(raw_noise, -clip_eta, clip_eta)\n",
    "            actions = actions + noise\n",
    "            actions = torch.clamp(actions, -1., 1.)\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.166671Z",
     "start_time": "2025-04-24T08:13:50.004540Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:22.720924Z",
     "iopub.status.busy": "2025-05-11T08:53:22.720533Z",
     "iopub.status.idle": "2025-05-11T08:53:23.929236Z",
     "shell.execute_reply": "2025-05-11T08:53:23.928554Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.990771Z",
     "start_time": "2025-04-24T08:13:50.988283Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.931689Z",
     "iopub.status.busy": "2025-05-11T08:53:23.931265Z",
     "iopub.status.idle": "2025-05-11T08:53:23.935795Z",
     "shell.execute_reply": "2025-05-11T08:53:23.935293Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.367535Z",
     "start_time": "2025-04-24T08:13:51.358427Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.938343Z",
     "iopub.status.busy": "2025-05-11T08:53:23.937915Z",
     "iopub.status.idle": "2025-05-11T08:53:23.945115Z",
     "shell.execute_reply": "2025-05-11T08:53:23.944474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.947700Z",
     "iopub.status.busy": "2025-05-11T08:53:23.947290Z",
     "iopub.status.idle": "2025-05-11T08:53:23.958044Z",
     "shell.execute_reply": "2025-05-11T08:53:23.957485Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_best_weights(actor_model, critic1_model, critic2_model, weights_dir, device):\n",
    "    \"\"\"\n",
    "    Загружает веса для актора и двух критиков из файлов с наивысшей оценкой.\n",
    "\n",
    "    Args:\n",
    "        actor_model (torch.nn.Module): Экземпляр модели актора.\n",
    "        critic1_model (torch.nn.Module): Экземпляр модели первого критика.\n",
    "        critic2_model (torch.nn.Module): Экземпляр модели второго критика.\n",
    "        weights_dir (str): Директория, где хранятся веса.\n",
    "        device (torch.device): Устройство (cpu или cuda), на которое загружать модели.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(weights_dir):\n",
    "        print(f\"Директория с весами '{weights_dir}' не найдена.\")\n",
    "        return False\n",
    "\n",
    "    # Ищем все файлы актора, чтобы извлечь оценки\n",
    "    actor_weight_files = glob.glob(os.path.join(weights_dir, '*_actor.pth'))\n",
    "\n",
    "    if not actor_weight_files:\n",
    "        print(f\"В директории '{weights_dir}' не найдены файлы весов актора.\")\n",
    "        return False\n",
    "\n",
    "    highest_score = -1\n",
    "    best_actor_path = None\n",
    "    best_critic1_path = None\n",
    "    best_critic2_path = None\n",
    "\n",
    "    print(\"Найденные файлы весов актора для анализа:\")\n",
    "    for f_path in actor_weight_files:\n",
    "        print(f\" - {f_path}\")\n",
    "        try:\n",
    "            # Извлекаем имя файла без пути: например, \"123_actor.pth\"\n",
    "            filename = os.path.basename(f_path)\n",
    "            # Извлекаем часть с оценкой: \"123\"\n",
    "            score_str = filename.split('_')[0]\n",
    "            score = int(score_str)\n",
    "\n",
    "            if score > highest_score:\n",
    "                highest_score = score\n",
    "                # Формируем пути для всех трех моделей с этой оценкой\n",
    "                best_actor_path = f_path\n",
    "                best_critic1_path = os.path.join(weights_dir, f\"{score}_critic1.pth\")\n",
    "                best_critic2_path = os.path.join(weights_dir, f\"{score}_critic2.pth\")\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Не удалось извлечь оценку из имени файла '{f_path}': {e}\")\n",
    "            continue # Пропускаем файлы с некорректным именем\n",
    "\n",
    "    if highest_score == -1:\n",
    "        print(\"Не найдено корректных файлов весов с оценками.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\nЗагрузка весов с наивысшей оценкой: {highest_score}\")\n",
    "\n",
    "    loaded_successfully = True\n",
    "\n",
    "    # Загрузка актора\n",
    "    if best_actor_path and os.path.exists(best_actor_path):\n",
    "        try:\n",
    "            actor_model.load_state_dict(torch.load(best_actor_path, map_location=device))\n",
    "            actor_model.eval() # Перевод в режим оценки\n",
    "            print(f\"Веса актора успешно загружены из: {best_actor_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке весов актора из {best_actor_path}: {e}\")\n",
    "            loaded_successfully = False\n",
    "    else:\n",
    "        print(f\"Файл весов актора для оценки {highest_score} не найден: {best_actor_path}\")\n",
    "        loaded_successfully = False\n",
    "\n",
    "    # Загрузка первого критика\n",
    "    if best_critic1_path and os.path.exists(best_critic1_path):\n",
    "        try:\n",
    "            critic1_model.load_state_dict(torch.load(best_critic1_path, map_location=device))\n",
    "            critic1_model.eval() # Перевод в режим оценки\n",
    "            print(f\"Веса критика 1 успешно загружены из: {best_critic1_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке весов критика 1 из {best_critic1_path}: {e}\")\n",
    "            loaded_successfully = False\n",
    "    else:\n",
    "        print(f\"Файл весов критика 1 для оценки {highest_score} не найден: {best_critic1_path}\")\n",
    "        loaded_successfully = False\n",
    "\n",
    "    # Загрузка второго критика\n",
    "    if best_critic2_path and os.path.exists(best_critic2_path):\n",
    "        try:\n",
    "            critic2_model.load_state_dict(torch.load(best_critic2_path, map_location=device))\n",
    "            critic2_model.eval() # Перевод в режим оценки\n",
    "            print(f\"Веса критика 2 успешно загружены из: {best_critic2_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке весов критика 2 из {best_critic2_path}: {e}\")\n",
    "            loaded_successfully = False\n",
    "    else:\n",
    "        print(f\"Файл весов критика 2 для оценки {highest_score} не найден: {best_critic2_path}\")\n",
    "        loaded_successfully = False\n",
    "\n",
    "    return loaded_successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.960164Z",
     "iopub.status.busy": "2025-05-11T08:53:23.959932Z",
     "iopub.status.idle": "2025-05-11T08:53:23.981416Z",
     "shell.execute_reply": "2025-05-11T08:53:23.980787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные файлы весов актора для анализа:\n",
      " - ./weights/TD3/83_actor.pth\n",
      " - ./weights/TD3/231_actor.pth\n",
      " - ./weights/TD3/75_actor.pth\n",
      " - ./weights/TD3/285_actor.pth\n",
      " - ./weights/TD3/256_actor.pth\n",
      " - ./weights/TD3/-122_actor.pth\n",
      " - ./weights/TD3/248_actor.pth\n",
      " - ./weights/TD3/310_actor.pth\n",
      " - ./weights/TD3/-114_actor.pth\n",
      " - ./weights/TD3/272_actor.pth\n",
      " - ./weights/TD3/-27_actor.pth\n",
      " - ./weights/TD3/-6_actor.pth\n",
      " - ./weights/TD3/-9_actor.pth\n",
      " - ./weights/TD3/152_actor.pth\n",
      " - ./weights/TD3/301_actor.pth\n",
      " - ./weights/TD3/300_actor.pth\n",
      " - ./weights/TD3/283_actor.pth\n",
      " - ./weights/TD3/-82_actor.pth\n",
      " - ./weights/TD3/-121_actor.pth\n",
      " - ./weights/TD3/-170_actor.pth\n",
      " - ./weights/TD3/-98_actor.pth\n",
      " - ./weights/TD3/261_actor.pth\n",
      " - ./weights/TD3/-109_actor.pth\n",
      " - ./weights/TD3/296_actor.pth\n",
      " - ./weights/TD3/-14_actor.pth\n",
      " - ./weights/TD3/-38_actor.pth\n",
      " - ./weights/TD3/196_actor.pth\n",
      " - ./weights/TD3/107_actor.pth\n",
      " - ./weights/TD3/-80_actor.pth\n",
      " - ./weights/TD3/209_actor.pth\n",
      " - ./weights/TD3/274_actor.pth\n",
      " - ./weights/TD3/0_actor.pth\n",
      " - ./weights/TD3/165_actor.pth\n",
      " - ./weights/TD3/295_actor.pth\n",
      " - ./weights/TD3/-133_actor.pth\n",
      " - ./weights/TD3/269_actor.pth\n",
      " - ./weights/TD3/43_actor.pth\n",
      " - ./weights/TD3/228_actor.pth\n",
      " - ./weights/TD3/-85_actor.pth\n",
      " - ./weights/TD3/53_actor.pth\n",
      " - ./weights/TD3/-154_actor.pth\n",
      " - ./weights/TD3/242_actor.pth\n",
      " - ./weights/TD3/306_actor.pth\n",
      " - ./weights/TD3/210_actor.pth\n",
      " - ./weights/TD3/-104_actor.pth\n",
      " - ./weights/TD3/-116_actor.pth\n",
      " - ./weights/TD3/-100_actor.pth\n",
      " - ./weights/TD3/-96_actor.pth\n",
      " - ./weights/TD3/27_actor.pth\n",
      " - ./weights/TD3/60_actor.pth\n",
      " - ./weights/TD3/-107_actor.pth\n",
      " - ./weights/TD3/-93_actor.pth\n",
      " - ./weights/TD3/190_actor.pth\n",
      " - ./weights/TD3/-86_actor.pth\n",
      " - ./weights/TD3/-102_actor.pth\n",
      " - ./weights/TD3/163_actor.pth\n",
      " - ./weights/TD3/6_actor.pth\n",
      " - ./weights/TD3/100_actor.pth\n",
      " - ./weights/TD3/289_actor.pth\n",
      "\n",
      "Загрузка весов с наивысшей оценкой: 310\n",
      "Веса актора успешно загружены из: ./weights/TD3/310_actor.pth\n",
      "Веса критика 1 успешно загружены из: ./weights/TD3/310_critic1.pth\n",
      "Веса критика 2 успешно загружены из: ./weights/TD3/310_critic2.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_best_weights(actor, critic1, critic2, \"./weights/TD3\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы стабилизировать обучение, нам понадобятся целевые сети &mdash; медленно обновляемые копии наших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.835845Z",
     "start_time": "2025-04-24T08:13:51.822962Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.983589Z",
     "iopub.status.busy": "2025-05-11T08:53:23.983375Z",
     "iopub.status.idle": "2025-05-11T08:53:23.991027Z",
     "shell.execute_reply": "2025-05-11T08:53:23.990526Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах с непрерывным управлением целевые сети обычно обновляются с использованием экспоненциального сглаживания:\n",
    "$$\\theta^{-} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^{-},$$\n",
    "где $\\theta^{-}$ &mdash; веса дополнительной сети, $\\theta$ &mdash; текущие параметры модели, а $\\tau$ &mdash; гиперпараметр (коэффициент обновления)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.256363Z",
     "start_time": "2025-04-24T08:13:52.250709Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.993305Z",
     "iopub.status.busy": "2025-05-11T08:53:23.992872Z",
     "iopub.status.idle": "2025-05-11T08:53:23.996432Z",
     "shell.execute_reply": "2025-05-11T08:53:23.995886Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_target_networks(model, target_model, tau=0.005):\n",
    "    for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, у нас будет три отдельных процедуры оптимизации для обучения трёх моделей, так что давайте поприветствуем наших трёх Адамов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.658128Z",
     "start_time": "2025-04-24T08:13:52.650846Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:23.998491Z",
     "iopub.status.busy": "2025-05-11T08:53:23.998267Z",
     "iopub.status.idle": "2025-05-11T08:53:26.354453Z",
     "shell.execute_reply": "2025-05-11T08:53:26.353805Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:53.063674Z",
     "start_time": "2025-04-24T08:13:53.057164Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:26.357483Z",
     "iopub.status.busy": "2025-05-11T08:53:26.356821Z",
     "iopub.status.idle": "2025-05-11T08:53:26.361393Z",
     "shell.execute_reply": "2025-05-11T08:53:26.360788Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(name, model, optimizer, loss, n_iterations, max_grad_norm=10):\n",
    "    '''\n",
    "    Выполняет один шаг оптимизации, ограничивает норму градиента значением max_grad_norm\n",
    "    и логирует всё в TensorBoard.\n",
    "    '''\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    env.writer.add_scalar(name, loss.item(), n_iterations)\n",
    "    env.writer.add_scalar(name + \"_grad_norm\", grad_norm.item(), n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для одного сэмплированного перехода $(s, a, r, s')$ целевое значение имеет вид:\n",
    "$$y(s, a) = r + \\gamma V(s').$$\n",
    "Как вычисляется $V(s')$? Формально, оценка Монте-Карло выглядит просто:\n",
    "$$V(s') \\approx Q(s', a'),$$\n",
    "где $a'$ &mdash; это сэмпл из текущей политики $\\pi(a' \\mid s')$.\n",
    "\n",
    "Однако наш актор $\\pi$ обучается выбирать такие действия $a'$, где критик выдаёт наибольшие значения, что может приводить к завышенным оценкам. Чтобы избежать этого, мы используем несколько приёмов:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1,2} \\left\\{Q^{-}_i(s', a')\\right\\},$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики: $\\pi^{-}(a' \\mid s')$.\n",
    "2. Для вычисления $a'$ используется режим с обрезанным шумом, чтобы не позволить политике эксплуатировать узкие пики в $Q$-функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.138205Z",
     "start_time": "2025-04-24T01:58:06.138199Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.158713Z",
     "start_time": "2025-04-24T08:13:54.154703Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:26.364069Z",
     "iopub.status.busy": "2025-05-11T08:53:26.363624Z",
     "iopub.status.idle": "2025-05-11T08:53:26.370191Z",
     "shell.execute_reply": "2025-05-11T08:53:26.369601Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### Ваш код\n",
    "        is_not_done = 1 - is_done\n",
    "        next_actions = target_actor.get_target_action(next_states)#std_noise=0.05, clip_eta=0.2)\n",
    "        next_actions = torch.tensor(next_actions, device=DEVICE)\n",
    "        q1 =  target_critic1.get_qvalues(next_states, next_actions)\n",
    "        q2 =  target_critic2.get_qvalues(next_states, next_actions)\n",
    "        critic_target = rewards + gamma * torch.min(q1, q2) * is_not_done\n",
    "        ###\n",
    "\n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения актора мы хотим просто максимизировать:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} \\left[Q(s, a)\\right] \\to \\max_{\\pi}.$$\n",
    "Так как политика детерминированная, математическое ожидание сводится к:\n",
    "$$Q(s, \\pi(s)) \\to \\max_{\\pi}.$$\n",
    "\n",
    "**Замечание:**  \n",
    "Мы будем использовать `critic1` в качестве функции $Q$, которую актор будет пытаться \"эксплуатировать\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.997518Z",
     "start_time": "2025-04-24T08:13:54.991477Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:26.372403Z",
     "iopub.status.busy": "2025-05-11T08:53:26.372192Z",
     "iopub.status.idle": "2025-05-11T08:53:26.375905Z",
     "shell.execute_reply": "2025-05-11T08:53:26.375362Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states, actor, critic1):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "    ### Ваш код\n",
    "    actions = actor.get_best_action(states)\n",
    "    q = critic1.get_qvalues(states, actions)\n",
    "    actor_loss = -q.mean()\n",
    "    ###\n",
    "\n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **300 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:55.748252Z",
     "start_time": "2025-04-24T08:13:55.741014Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:26.377944Z",
     "iopub.status.busy": "2025-05-11T08:53:26.377750Z",
     "iopub.status.idle": "2025-05-11T08:53:26.385442Z",
     "shell.execute_reply": "2025-05-11T08:53:26.384792Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 4242 # Иногда может сильно не повезти\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:57.684751Z",
     "start_time": "2025-04-24T08:13:56.642708Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:26.387689Z",
     "iopub.status.busy": "2025-05-11T08:53:26.387276Z",
     "iopub.status.idle": "2025-05-11T08:53:28.408638Z",
     "shell.execute_reply": "2025-05-11T08:53:28.407777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ebfd76f9fb92043b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ebfd76f9fb92043b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:14:02.418310Z",
     "start_time": "2025-04-24T08:14:02.404628Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:28.412285Z",
     "iopub.status.busy": "2025-05-11T08:53:28.411924Z",
     "iopub.status.idle": "2025-05-11T08:53:28.428542Z",
     "shell.execute_reply": "2025-05-11T08:53:28.427624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=1500000, timesteps_per_epoch=1, start_timesteps = 5000,\n",
    "            batch_size=128, policy_update_freq=2, gamma=0.99, tau=0.005):\n",
    "\n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    ### Мой код\n",
    "    mse = nn.MSELoss()\n",
    "    last_20_rewards = deque(maxlen=20)\n",
    "    reward_sum = 0\n",
    "    r = 0\n",
    "    ###\n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "\n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        reward, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "        ### Мой код\n",
    "        if n_iterations % 10_000 == 0:\n",
    "            r = evaluate(eval_env, actor, 20).mean()\n",
    "            print(f\"saving with {r=}\")\n",
    "            torch.save(actor.state_dict(), f'weights/TD3/{int(r)}_actor.pth')\n",
    "            torch.save(critic1.state_dict(), f'weights/TD3/{int(r)}_critic1.pth')\n",
    "            torch.save(critic2.state_dict(), f'weights/TD3/{int(r)}_critic2.pth')\n",
    "\n",
    "            if r > 310:\n",
    "                print(\"WE WON !!!\")\n",
    "                break\n",
    "        ###\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_computed = compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2, gamma)\n",
    "\n",
    "        q_crititc1 = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = mse(q_crititc1, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_crititc2 = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = mse(q_crititc2, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            ### Ваш код\n",
    "            actor_loss = compute_actor_loss(states, actor, critic1)\n",
    "            ###\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)\n",
    "            update_target_networks(actor, target_actor, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:15.155234Z",
     "start_time": "2025-04-24T08:14:03.190411Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T08:53:28.431637Z",
     "iopub.status.busy": "2025-05-11T08:53:28.431294Z",
     "iopub.status.idle": "2025-05-11T09:35:55.874542Z",
     "shell.execute_reply": "2025-05-11T09:35:55.873655Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7024b98e4b437da82c879c88295ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3587/1980648776.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_actions = torch.tensor(next_actions, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(210.77461)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-170.00507)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-114.62284)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-82.25016)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(75.16844)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(256.95706)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(285.22266)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(300.49237)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(165.85086)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-122.436386)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-133.85867)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-154.09146)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-98.59237)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(272.2517)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(289.22552)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(27.03866)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-116.263466)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(274.27307)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(6.3958254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(107.762405)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(283.4815)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(310.7676)\n",
      "WE WON !!!\n"
     ]
    }
   ],
   "source": [
    "run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.141979Z",
     "start_time": "2025-04-24T01:58:06.141974Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:19.633149Z",
     "start_time": "2025-04-24T08:42:17.757596Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:35:55.877017Z",
     "iopub.status.busy": "2025-05-11T09:35:55.876551Z",
     "iopub.status.idle": "2025-05-11T09:36:07.766318Z",
     "shell.execute_reply": "2025-05-11T09:36:07.765708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша награда: 310.55047607421875\n",
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша награда: {score}\")\n",
    "\n",
    "try:\n",
    "    assert score >= 300, \"Нужно больше учить?\"\n",
    "    print(\"Отлично!\")\n",
    "except:\n",
    "    print(\"не получилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:07.768526Z",
     "iopub.status.busy": "2025-05-11T09:36:07.768296Z",
     "iopub.status.idle": "2025-05-11T09:36:07.776079Z",
     "shell.execute_reply": "2025-05-11T09:36:07.775571Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(actor.state_dict(), f'weights/TD3/{int(score)}_actor.pth')\n",
    "torch.save(critic1.state_dict(), f'weights/TD3/{int(score)}_critic1.pth')\n",
    "torch.save(critic2.state_dict(), f'weights/TD3/{int(score)}_critic2.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.976927Z",
     "start_time": "2025-04-24T08:42:22.393814Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:07.778369Z",
     "iopub.status.busy": "2025-05-11T09:36:07.778142Z",
     "iopub.status.idle": "2025-05-11T09:36:11.808547Z",
     "shell.execute_reply": "2025-05-11T09:36:11.807946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/videos_TD3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_TD3\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.980665Z",
     "start_time": "2025-04-24T08:42:23.978191Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:11.810946Z",
     "iopub.status.busy": "2025-05-11T09:36:11.810716Z",
     "iopub.status.idle": "2025-05-11T09:36:11.816564Z",
     "shell.execute_reply": "2025-05-11T09:36:11.815934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_TD3/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_TD3').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:31.821856Z",
     "start_time": "2025-04-24T08:59:31.818368Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:11.818929Z",
     "iopub.status.busy": "2025-05-11T09:36:11.818346Z",
     "iopub.status.idle": "2025-05-11T09:36:11.824986Z",
     "shell.execute_reply": "2025-05-11T09:36:11.824380Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"SAC\")\n",
    "\n",
    "eval_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно смоделировать гауссовскую политику. Это означает, что распределение политики &mdash; это многомерное нормальное распределение с диагональной ковариационной матрицей. Должны предсказываться среднее и ковариация, при этом важно гарантировать, что ковариация остаётся неотрицательной. Пусть $f_{\\theta}(s)$ &mdash; выход головы ковариации, преобразуем выход в диапазон $[-1, 1]$ с помощью `tanh`, затем спроецируем результат в интервал $[m, M]$, где $m = -20$, $M = 2$, и применим экспоненту. Это обеспечит адекватный диапазон ковариации:\n",
    "$$\\sigma(s) = \\exp^{m + 0.5(M - m)(\\tanh(f_{\\theta}(s)) + 1)}.$$\n",
    "\n",
    "Гауссовское распределение не ограничено, но нужно, чтобы действия лежали в диапазоне $[-1, 1]$. Для этого:\n",
    "1. Моделируем неограниченное распределение:  \n",
    "   $\\mathcal{N}(\\mu_{\\theta}(s), \\sigma_{\\theta}(s)^2I)$\n",
    "2. Затем каждую выборку $u$ из этого распределения преобразуем с помощью $\\tanh$:\n",
    "$$u \\sim \\mathcal{N}(\\mu, \\sigma^2I)$$\n",
    "$$a = \\tanh(u)$$\n",
    "\n",
    "**Важно:** После применения $\\tanh$ необходимо использовать формулу замены переменных при вычислении логарифма плотности вероятности:\n",
    "$$\\log p(a \\mid \\mu, \\sigma) = \\log p(u \\mid \\mu, \\sigma) - \\sum_{i = 1}^D \\log \\left(1 - \\tanh^2(u_i)\\right),$$\n",
    "где $D$ &mdash; размерность действия (`action_dim`). На практике желательно добавить небольшое значение (например, $1e{-6}$) внутрь логарифма для числовой устойчивости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:36.275645Z",
     "start_time": "2025-04-24T08:59:36.272221Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:11.827124Z",
     "iopub.status.busy": "2025-05-11T09:36:11.826893Z",
     "iopub.status.idle": "2025-05-11T09:36:11.835576Z",
     "shell.execute_reply": "2025-05-11T09:36:11.834928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAC_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256, m=-20, M=2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.m = m\n",
    "        self.M = M\n",
    "\n",
    "        ### Ваш код\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.std_head = nn.Linear(hidden_dim, action_dim)\n",
    "        ###\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        ### Ваш код\n",
    "        x = self.base(state)\n",
    "        mean = self.mean_head(x)\n",
    "\n",
    "        raw_std = self.std_head(x)\n",
    "        scale = torch.tanh(raw_std)\n",
    "        scale = self.m + 0.5 * (self.M - self.m) * (scale + 1)\n",
    "        cov = torch.exp(scale)\n",
    "        ###\n",
    "\n",
    "        return mean, cov\n",
    "\n",
    "    def sample(self, states):\n",
    "        '''\n",
    "        Сэмплирует действия.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "            states: torch.tensor [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "            actions: torch.tensor [batch_size x actions_dim]\n",
    "            log_prob: torch.tensor [batch_size]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        mean, std = self.forward(states)\n",
    "        dist = Normal(mean, std)\n",
    "        u = dist.rsample()\n",
    "        actions = torch.tanh(u)\n",
    "        log_prob = dist.log_prob(u)\n",
    "        log_prob -= (1 - actions.pow(2) + 1e-6).log()\n",
    "        log_prob = log_prob.sum(dim=-1)\n",
    "        ###\n",
    "\n",
    "        return actions, log_prob\n",
    "\n",
    "    def get_action(self, states):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            states = torch.tensor(states, device=self.device, dtype=torch.float32)\n",
    "            actions, _ = self.sample(states)\n",
    "            actions = actions.cpu().numpy()\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:39.079174Z",
     "start_time": "2025-04-24T08:59:38.889928Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:11.838121Z",
     "iopub.status.busy": "2025-05-11T09:36:11.837719Z",
     "iopub.status.idle": "2025-05-11T09:36:13.189831Z",
     "shell.execute_reply": "2025-05-11T09:36:13.189075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.091591Z",
     "start_time": "2025-04-24T08:59:41.089590Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.192530Z",
     "iopub.status.busy": "2025-05-11T09:36:13.192066Z",
     "iopub.status.idle": "2025-05-11T09:36:13.196111Z",
     "shell.execute_reply": "2025-05-11T09:36:13.195612Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 10**5\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.582761Z",
     "start_time": "2025-04-24T08:59:41.579812Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.198229Z",
     "iopub.status.busy": "2025-05-11T09:36:13.198032Z",
     "iopub.status.idle": "2025-05-11T09:36:13.205453Z",
     "shell.execute_reply": "2025-05-11T09:36:13.204936Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.207807Z",
     "iopub.status.busy": "2025-05-11T09:36:13.207399Z",
     "iopub.status.idle": "2025-05-11T09:36:13.212628Z",
     "shell.execute_reply": "2025-05-11T09:36:13.212096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные файлы весов актора для анализа:\n",
      " - ./weights/SAC/-56_actor.pth\n",
      " - ./weights/SAC/-78_actor.pth\n",
      "Не найдено корректных файлов весов с оценками.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_best_weights(actor, critic1, critic2, \"./weights/SAC\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:42.130567Z",
     "start_time": "2025-04-24T08:59:42.127553Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.215055Z",
     "iopub.status.busy": "2025-05-11T09:36:13.214641Z",
     "iopub.status.idle": "2025-05-11T09:36:13.221045Z",
     "shell.execute_reply": "2025-05-11T09:36:13.220501Z"
    }
   },
   "outputs": [],
   "source": [
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:50.695410Z",
     "start_time": "2025-04-24T08:59:50.686991Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.223529Z",
     "iopub.status.busy": "2025-05-11T09:36:13.223103Z",
     "iopub.status.idle": "2025-05-11T09:36:13.227217Z",
     "shell.execute_reply": "2025-05-11T09:36:13.226698Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1, 2} \\left\\{Q^{-}_i(s', a')\\right\\},$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики $\\pi(a' \\mid s')$;\n",
    "2. Добавляется энтропийный бонус:\n",
    "$$V(s') = \\min_{i = 1, 2} \\left\\{Q^{-}_i(s', a')\\right\\} - \\alpha \\log \\pi(a' \\mid s').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.145091Z",
     "start_time": "2025-04-24T01:58:06.145086Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:58.722072Z",
     "start_time": "2025-04-24T08:59:58.712927Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.229731Z",
     "iopub.status.busy": "2025-05-11T09:36:13.229270Z",
     "iopub.status.idle": "2025-05-11T09:36:13.233736Z",
     "shell.execute_reply": "2025-05-11T09:36:13.233172Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done, actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "        alpha: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### Ваш код\n",
    "        is_not_done = 1 - is_done\n",
    "        next_actions, log_prob = actor.sample(next_states)\n",
    "        q1 = target_critic1.get_qvalues(next_states, next_actions)\n",
    "        q2 = target_critic2.get_qvalues(next_states, next_actions)\n",
    "        min_q = torch.min(q1, q2)\n",
    "        V = min_q - alpha * log_prob\n",
    "        critic_target = rewards + gamma * V * is_not_done\n",
    "        ###\n",
    "\n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "Добавляется регуляризатор энтропии, чтобы стимулировать стохастичность политики:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} \\left[Q(s, a) - \\alpha \\log \\pi(a \\mid s)\\right] \\to \\max_{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:01.896966Z",
     "start_time": "2025-04-24T09:00:01.892117Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.236243Z",
     "iopub.status.busy": "2025-05-11T09:36:13.235812Z",
     "iopub.status.idle": "2025-05-11T09:36:13.239591Z",
     "shell.execute_reply": "2025-05-11T09:36:13.239064Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states, actor, critic1, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        alpha: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "    ### Ваш код\n",
    "    actions, log_prob = actor.sample(states)\n",
    "    q = critic1.get_qvalues(states, actions)\n",
    "    actor_loss = (-q + alpha * log_prob).mean()\n",
    "    ###\n",
    "\n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **250 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:10.972995Z",
     "start_time": "2025-04-24T09:00:10.965573Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.242000Z",
     "iopub.status.busy": "2025-05-11T09:36:13.241539Z",
     "iopub.status.idle": "2025-05-11T09:36:13.247440Z",
     "shell.execute_reply": "2025-05-11T09:36:13.246786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7aa8c6aeffd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Иногда может сильно не повезти\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:45:08.879108Z",
     "start_time": "2025-04-24T10:45:05.325186Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.249940Z",
     "iopub.status.busy": "2025-05-11T09:36:13.249499Z",
     "iopub.status.idle": "2025-05-11T09:36:13.259091Z",
     "shell.execute_reply": "2025-05-11T09:36:13.258444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 3667), started 0:42:45 ago. (Use '!kill 3667' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-58e285a3a5219eaa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-58e285a3a5219eaa\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:28:37.626011Z",
     "start_time": "2025-04-24T10:28:37.621755Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.261554Z",
     "iopub.status.busy": "2025-05-11T09:36:13.261140Z",
     "iopub.status.idle": "2025-05-11T09:36:13.270575Z",
     "shell.execute_reply": "2025-05-11T09:36:13.269900Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=1000000, timesteps_per_epoch=1, start_timesteps = 5000,\n",
    "            batch_size=128, policy_update_freq=2, gamma=0.99, tau=0.005, alpha=0.1):\n",
    "\n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    ### Мой код\n",
    "    mse = nn.MSELoss()\n",
    "    last_100_rewards = deque(maxlen=100)\n",
    "    ###\n",
    "\n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "\n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        reward, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        ### Мой код\n",
    "        if n_iterations % 10_000 == 0:\n",
    "            r = evaluate(eval_env, actor, 20).mean()\n",
    "            print(f\"saving with {r=}\")\n",
    "            torch.save(actor.state_dict(), f'weights/TD3/{int(r)}_actor.pth')\n",
    "            torch.save(critic1.state_dict(), f'weights/TD3/{int(r)}_critic1.pth')\n",
    "            torch.save(critic2.state_dict(), f'weights/TD3/{int(r)}_critic2.pth')\n",
    "\n",
    "            if r > 260:\n",
    "                print(\"WE WON !!!\")\n",
    "                break\n",
    "        ###\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_computed = compute_critic_target(rewards, next_states, is_done, actor, target_critic1, target_critic2, gamma, alpha)\n",
    "\n",
    "        q_crititc1 = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = mse(q_crititc1, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_crititc2 = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = mse(q_crititc2, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            ### Ваш код\n",
    "            critic = [critic1, critic2][random.randint(0,1)]\n",
    "            actor_loss = compute_actor_loss(states, actor, critic, alpha)\n",
    "            ###\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:44:15.948407Z",
     "start_time": "2025-04-24T10:28:38.990752Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T09:36:13.273010Z",
     "iopub.status.busy": "2025-05-11T09:36:13.272576Z",
     "iopub.status.idle": "2025-05-11T10:51:40.178829Z",
     "shell.execute_reply": "2025-05-11T10:51:40.178080Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ccb322e2f4b5fbc3b82f9d3b3e0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-109.76815)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-93.53387)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-100.727234)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-85.469215)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-86.58558)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-80.79251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-96.64775)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-107.69502)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-109.61479)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-107.927536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(0.0995492)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-104.90053)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-121.47394)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-102.94299)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-14.217276)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-27.003836)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(43.755165)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-9.899195)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-6.480212)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(-38.819645)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(60.20571)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(53.67806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(53.28736)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(75.280624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(83.72656)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(100.663795)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(163.447)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(209.10873)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(231.9719)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(242.74199)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving with r=np.float32(269.5849)\n",
      "WE WON !!!\n"
     ]
    }
   ],
   "source": [
    "run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.147830Z",
     "start_time": "2025-04-24T01:58:06.147825Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:46:58.036798Z",
     "start_time": "2025-04-24T10:46:52.730916Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T10:51:40.181388Z",
     "iopub.status.busy": "2025-05-11T10:51:40.180916Z",
     "iopub.status.idle": "2025-05-11T10:52:08.716482Z",
     "shell.execute_reply": "2025-05-11T10:52:08.715843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша награда: 288.6072082519531\n",
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша награда: {score}\")\n",
    "\n",
    "try:\n",
    "    assert score >= 250, \"Нужно больше учить?\"\n",
    "    print(\"Отлично!\")\n",
    "except:\n",
    "    print(\"не получилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:52:08.719026Z",
     "iopub.status.busy": "2025-05-11T10:52:08.718405Z",
     "iopub.status.idle": "2025-05-11T10:52:08.727279Z",
     "shell.execute_reply": "2025-05-11T10:52:08.726730Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), f'weights/SAC/{int(score)}_actor.pth')\n",
    "torch.save(critic1.state_dict(), f'weights/SAC/{int(score)}_critic1.pth')\n",
    "torch.save(critic2.state_dict(), f'weights/SAC/{int(score)}_critic2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.332491Z",
     "start_time": "2025-04-24T10:27:43.771771Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T10:52:08.729639Z",
     "iopub.status.busy": "2025-05-11T10:52:08.729218Z",
     "iopub.status.idle": "2025-05-11T10:52:12.844715Z",
     "shell.execute_reply": "2025-05-11T10:52:12.844009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/videos_SAC folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_SAC\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.336278Z",
     "start_time": "2025-04-24T10:27:45.333699Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T10:52:12.847368Z",
     "iopub.status.busy": "2025-05-11T10:52:12.846857Z",
     "iopub.status.idle": "2025-05-11T10:52:12.852539Z",
     "shell.execute_reply": "2025-05-11T10:52:12.851903Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_SAC/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_SAC').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(5 баллов)__ </span> Выбить хотя бы **300 награды** в режиме хардкора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:52:12.854647Z",
     "iopub.status.busy": "2025-05-11T10:52:12.854440Z",
     "iopub.status.idle": "2025-05-11T10:52:12.859925Z",
     "shell.execute_reply": "2025-05-11T10:52:12.859324Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"Hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные файлы весов актора для анализа:\n",
      " - ./weights/SAC/-56_actor.pth\n",
      " - ./weights/SAC/-78_actor.pth\n",
      " - ./weights/SAC/288_actor.pth\n",
      "\n",
      "Загрузка весов с наивысшей оценкой: 288\n",
      "Веса актора успешно загружены из: ./weights/SAC/288_actor.pth\n",
      "Веса критика 1 успешно загружены из: ./weights/SAC/288_critic1.pth\n",
      "Веса критика 2 успешно загружены из: ./weights/SAC/288_critic2.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_best_weights(actor, critic1, critic2, \"./weights/SAC\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T10:52:12.862487Z",
     "iopub.status.busy": "2025-05-11T10:52:12.861985Z",
     "iopub.status.idle": "2025-05-11T10:52:12.865074Z",
     "shell.execute_reply": "2025-05-11T10:52:12.864458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша награда: -58.50016403198242\n",
      "не получилось\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша награда: {score}\")\n",
    "\n",
    "try:\n",
    "    assert score >= 250, \"Нужно больше учить?\"\n",
    "    print(\"Отлично!\")\n",
    "except:\n",
    "    print(\"не получилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/lotfullka/Experiments/ReinforcedLearning/RL-ontinuous-ontrol-Lab/videos_HARD folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_HARD\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_HARD/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_HARD').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "003de6bb99f94c6783dd02cee6d22e6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ad8665169df4b238ca70444d993dadb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2eca43bc98c04a61bf0fd206cb7d304e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "303ae5b09d4741e5b7e819c8770206cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39adf9992125464188c7419d364d83f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3cd3be054b634e8c9fd27724428ef5cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "599438b077fe4374beeb282037e3434e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62fe125eee8647c780f3161a1e1b73d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69714d1f2c54412fb8176c760ebd8fc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "76af448e576143b9ad0b2f2e04448992": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dd3e06d8f5014062b15d416a22dec981",
       "max": 1500000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3cd3be054b634e8c9fd27724428ef5cc",
       "tabbable": null,
       "tooltip": null,
       "value": 220000
      }
     },
     "7e6ccb322e2f4b5fbc3b82f9d3b3e0be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f2826fcc558044589b201e3390950908",
        "IPY_MODEL_927fd9a8a1764ab489e6f012f57d9765",
        "IPY_MODEL_f3fb02a498fb471cadb0951ceaf281e4"
       ],
       "layout": "IPY_MODEL_d327795c1f144724815796df44873e38",
       "tabbable": null,
       "tooltip": null
      }
     },
     "927fd9a8a1764ab489e6f012f57d9765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b27c53e976824a1da142104c8c07d8ce",
       "max": 1000000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_69714d1f2c54412fb8176c760ebd8fc8",
       "tabbable": null,
       "tooltip": null,
       "value": 310000
      }
     },
     "9774cc758423466d81e26339daebd3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b27c53e976824a1da142104c8c07d8ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bcf43a57ca674eb1a187ed70ed89136b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_62fe125eee8647c780f3161a1e1b73d2",
       "placeholder": "​",
       "style": "IPY_MODEL_1ad8665169df4b238ca70444d993dadb",
       "tabbable": null,
       "tooltip": null,
       "value": " 220000/1500000 [42:27&lt;3:53:47, 91.25it/s]"
      }
     },
     "cd7024b98e4b437da82c879c88295ddd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e97a58a01bbd439ca3c73ae5b3f47ddf",
        "IPY_MODEL_76af448e576143b9ad0b2f2e04448992",
        "IPY_MODEL_bcf43a57ca674eb1a187ed70ed89136b"
       ],
       "layout": "IPY_MODEL_db83f7f367dd43b8bdd66e995849f37f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d327795c1f144724815796df44873e38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db83f7f367dd43b8bdd66e995849f37f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd3e06d8f5014062b15d416a22dec981": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e97a58a01bbd439ca3c73ae5b3f47ddf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_003de6bb99f94c6783dd02cee6d22e6f",
       "placeholder": "​",
       "style": "IPY_MODEL_39adf9992125464188c7419d364d83f0",
       "tabbable": null,
       "tooltip": null,
       "value": " 15%"
      }
     },
     "f2826fcc558044589b201e3390950908": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_599438b077fe4374beeb282037e3434e",
       "placeholder": "​",
       "style": "IPY_MODEL_2eca43bc98c04a61bf0fd206cb7d304e",
       "tabbable": null,
       "tooltip": null,
       "value": " 31%"
      }
     },
     "f3fb02a498fb471cadb0951ceaf281e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_303ae5b09d4741e5b7e819c8770206cc",
       "placeholder": "​",
       "style": "IPY_MODEL_9774cc758423466d81e26339daebd3b9",
       "tabbable": null,
       "tooltip": null,
       "value": " 310000/1000000 [1:15:26&lt;2:22:21, 80.79it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
