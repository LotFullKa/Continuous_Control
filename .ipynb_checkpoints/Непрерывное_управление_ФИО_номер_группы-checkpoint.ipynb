{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T22:46:50.460493Z",
     "start_time": "2024-05-04T22:46:50.452158Z"
    }
   },
   "source": [
    "# Управление с непрерывными действиями (Continuous Control) (<span style=\"color: green\">10 баллов за основную часть + 5 баллов за бонусную часть</span>)\n",
    "\n",
    "#### Дедлайн (жёсткий) задания: 04.05.2025,  UTC+3.\n",
    "\n",
    "#### При сдаче задания нужно  поместить в архив данный файл и папки с логами и видео, сохраняя относительные пути, и послать архив в систему сдачи.\n",
    "\n",
    "### <span style=\"color: red\"> Если работа была списана и/или сделана LLM, то за работу ставится 0 баллов. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Лотфуллин Камиль Рашитович, М05-401."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании предлагается решить задачу управления с непрерывными действиями, используя алгоритмы:\n",
    "\n",
    "- **Twin Delayed DDPG (TD3)** (**Раздел 6.1.6 (Алгоритм 23)**)\n",
    "- **Soft Actor-Critic (SAC)** (**Раздел 6.2.4 (Алгоритм 24)**)\n",
    "\n",
    "Оба алгоритма являются off-policy и считаются одними из наиболее эффективных для задач управления в непрерывном пространстве действий. Они основаны на базовом алгоритме **Deep Deterministic Policy Gradient (DDPG)** (**Раздел 6.1.5 (Алгоритм 22)**), который можно представить как \"DQN с отдельной нейросетью для аппроксимации жадной политики\". Основные отличия заключаются в различных стабилизационных приёмах:\n",
    "\n",
    "- TD3 обучает детерминированную политику, тогда как SAC использует стохастическую политику. Это означает, что в SAC достаточно просто сэмплировать действия из политики для исследования, в то время как в TD3 необходимо вручную добавлять шум к действиям.\n",
    "- TD3 добавляет к действиям обрезанный шум (clipped noise) при расчёте целевых значений, что помогает бороться с переоценкой. SAC использует формализм максимальной энтропии и добавляет бонус за энтропию в функцию ценности, поощряя более разнообразные действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.167674Z",
     "start_time": "2025-04-24T08:13:43.161443Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from queue import deque\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from logger import TensorboardSummaries as Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.374789Z",
     "start_time": "2025-04-24T08:13:43.369879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Среда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы создадим экземпляр среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:43.931381Z",
     "start_time": "2025-04-24T08:13:43.916456Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность вектора состояний dim = 24\n",
      "n_actions = 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"Размерность вектора состояний dim =\", state_dim)\n",
    "print(\"n_actions =\", action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на среду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.371255Z",
     "start_time": "2025-04-24T08:13:44.271132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на случайную политику и посмотрим, как это выглядит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:44.637836Z",
     "start_time": "2025-04-24T08:13:44.636248Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomActor():\n",
    "    def get_action(self, states):\n",
    "        assert len(states.shape) == 1, \"Не работает с батчами\"\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.835552Z",
     "start_time": "2025-04-24T08:13:44.806511Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  1599\n",
      "done:  3199\n",
      "done:  3287\n",
      "done:  4887\n",
      "done:  5022\n",
      "done:  5078\n",
      "done:  5182\n",
      "done:  5311\n",
      "done:  5371\n",
      "done:  5417\n",
      "done:  5482\n",
      "done:  5566\n",
      "done:  7166\n",
      "done:  7303\n",
      "done:  7385\n",
      "done:  8985\n",
      "done:  9039\n",
      "done:  9088\n",
      "done:  9145\n",
      "done:  9209\n",
      "done:  9260\n",
      "done:  9355\n"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "rewards_per_step = []\n",
    "actor = RandomActor()\n",
    "\n",
    "for i in range(10000):\n",
    "    a = actor.get_action(s)\n",
    "    s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "    rewards_per_step.append(r)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        print(\"done: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основном, каждый эпизод длится **1600 шагов** &mdash; это ограничение по времени, после которого происходит завершение эпизода. Однако иногда эпизод завершается раньше, если симуляция \"понимает\", что агент, например, **упал** или **разбился** &mdash; то есть ситуация явно неудачная для продолжения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что мы получаем при использовании случайной политики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:45.839119Z",
     "start_time": "2025-04-24T08:13:45.836680Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, n_games=1, t_max=1600):\n",
    "    '''\n",
    "    Запускает n_games эпизодов и возвращает массив наград.\n",
    "\n",
    "    Возвращает\n",
    "    -------\n",
    "    rewards: np.array\n",
    "        Массив наград\n",
    "    '''\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        R = 0\n",
    "\n",
    "        for _ in range(t_max):\n",
    "\n",
    "            ### Ваш код\n",
    "            action = actor.get_action(s)\n",
    "            ###\n",
    "\n",
    "            assert (action.max() <= 1).all() and  (action.min() >= -1).all()\n",
    "\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.180382Z",
     "start_time": "2025-04-24T08:13:45.839786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lotfullin/Experiments/ReinforcedLearning/Непрерывное_управление/.venv/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/lotfullin/Experiments/ReinforcedLearning/Непрерывное_управление/videos_test folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_test\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.184346Z",
     "start_time": "2025-04-24T08:13:47.181920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_test/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_test').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Буфер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, что и в DQN. Вы можете просто скопировать код из вашего задания по DQN.\n",
    "\n",
    "#### Напомним интерфейс:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)`  &mdash; сохраняет кортеж (s,a,r,s',done) в буфер.\n",
    "* `exp_replay.sample(batch_size)` &mdash; возвращает наблюдения, действия, награды, следующие наблюдения и is_done для `batch_size` случайных сэмплов.\n",
    "* `len(exp_replay)` &mdash; возвращает количество элементов, хранящихся в буфере на данный момент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.196890Z",
     "start_time": "2025-04-24T08:13:47.184939Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Создаёт буфер реплеев.\n",
    "        \"\"\"\n",
    "        self._storage = deque(maxlen=size)\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add_random(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Убедитесь, что _storage не превзойдёт по размерам _maxsize.\n",
    "        Убедитесь, что FIFO правило выполняется: старейшие прецеденты должны удаляться раньше всех.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        self._storage.append(data)\n",
    "\n",
    "    def sample_random(self, batch_size):\n",
    "        \"\"\"\n",
    "        Сэмплирование батча переходов.\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            батч наблюдений (состояний)\n",
    "        act_batch: np.array\n",
    "            батч действий, выполненных на основе obs_batch\n",
    "        rew_batch: np.array\n",
    "            награды, полученные в качестве результата выполнения act_batch\n",
    "        next_obs_batch: np.array\n",
    "            следующие наблюдения (состояния), полученные после выполнения act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1, если выполнение act_batch[i] повлекло\n",
    "            окончание эпизода и 0 иначе.\n",
    "        \"\"\"\n",
    "\n",
    "        #< случайно сгенерировать batch_size индексов сэмплов в буфере >\n",
    "        indices = np.random.choice(len(self._storage), batch_size, replace=False)\n",
    "\n",
    "        # собрать <s,a,r,s',done> для каждого индекса\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for idx in indices:\n",
    "            sample = self._storage[idx]\n",
    "            states.append(sample[0])\n",
    "            actions.append(sample[1])\n",
    "            rewards.append(sample[2])\n",
    "            next_states.append(sample[3])\n",
    "            dones.append(sample[4])\n",
    "        # < states > , < actions >, < rewards >,  < next_states >, < is_done >\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.271903Z",
     "start_time": "2025-04-24T08:13:47.197662Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add_random(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"Размер буфера должен быть равен 10, потому что это максимальная вместимость\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для записи в буфер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.274946Z",
     "start_time": "2025-04-24T08:13:47.272633Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Играется в среде ровно n шагов и записывается каждая пятёрка (s, a, r, s', done) в буффер.\n",
    "\n",
    "    Возвращает\n",
    "    -------\n",
    "        sum_rewards: float\n",
    "            суммарную награду за n шагов.\n",
    "        s: float\n",
    "            состояние, в котором осталась среда.\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(n_steps):\n",
    "\n",
    "        ### Ваш код\n",
    "        a = agent.get_action(s)\n",
    "        ###\n",
    "\n",
    "        ns, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        exp_replay.add_random(s, a, r, ns, terminated)\n",
    "\n",
    "        s = env.reset()[0] if terminated or truncated else ns\n",
    "\n",
    "        sum_rewards += r\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:54:30.667197Z",
     "start_time": "2025-04-23T20:54:30.663277Z"
    }
   },
   "source": [
    "# Критик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте начнём с модели критика &mdash; она одинаковая для **TD3** и **SAC**. С одной стороны, он будет приближать оптимальную функцию $Q^*(s, a)$, а с другой &mdash; оценивать текущего актора $\\pi$, то есть рассматриваться как \n",
    "$Q^{\\pi}(s, a)$. Этот критик принимает на вход как состояние $s$, так и действие $a,$ а на выходе выдаёт скалярное значение.\n",
    "\n",
    "Важно: если модель возвращает скаляр на выходе, хорошей практикой является применение .squeeze(), чтобы избежать неожиданного broadcast-а, поскольку тензор формы [batch_size, 1] может автоматически расширяться при операциях с другими тензорами.\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейронная сеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(1 балл)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:47.907009Z",
     "start_time": "2025-04-24T08:13:47.895580Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Ваш код\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        ###\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "\n",
    "        ### Ваш код\n",
    "        x = torch.cat([states, actions], dim=-1)\n",
    "        output = self.net(x)\n",
    "        ###\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_qvalues(self, states, actions):\n",
    "        '''\n",
    "        Возвращает qvalues\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        qvalues: torch.tensor [batch_size]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        qvalues = self.forward(states, actions).squeeze(-1)\n",
    "        ###\n",
    "\n",
    "        assert len(qvalues.shape) == 1 and qvalues.shape[0] == states.shape[0]\n",
    "\n",
    "        return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:48.310526Z",
     "start_time": "2025-04-24T08:13:48.301042Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"TD3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим политику, или актора $\\pi$. Необходимо смоделировать детерминированную политику. То есть, модель должна возвращать `action_dim` чисел в диапазоне $[-1, 1]$. К сожалению, детерминированные политики могут вызывать проблемы со стабильностью и исследованием, поэтому нам потребуется реализовать три \"режима\" работы этой политики:\n",
    "\n",
    "1. Первый режим &mdash; **жадный** &mdash; это просто прямой проход через сеть. Он будет использоваться для обучения актора.\n",
    "2. Второй режим &mdash; **режим исследования** &mdash; когда нужно добавить шум (например, гауссовский), чтобы собирать более разнообразные данные.\n",
    "3. Третий режим &mdash; **\"обрезанный шум\"** &mdash; используется при расчёте целевого значения для критика. Здесь мы хотим добавить шум к выходу актора, но не слишком сильный, поэтому используем обрезанный шум:\n",
    "$$\\pi_{\\theta}(s) + \\varepsilon, \\quad \\varepsilon = \\operatorname{clip}(\\epsilon, -0.5, 0.5), \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "Рекомендуемая архитектура &mdash; полносвязная нейросеть (MLP) с тремя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:49.201849Z",
     "start_time": "2025-04-24T08:13:49.191425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TD3_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        ### Ваш код\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ###\n",
    "\n",
    "    def forward(self, state):\n",
    "        ### Ваш код\n",
    "        output = self.net(state)\n",
    "        ###\n",
    "        return output\n",
    "\n",
    "    def get_best_action(self, states):\n",
    "        '''\n",
    "        Используется для оптимизации актора.\n",
    "        Требуется, чтобы действия были дифференцируемыми по параметрам модели.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        actions = self.forward(states)\n",
    "        ###\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, states, std_noise=0.1):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "        Поэтому к действиям необходимо добавлять шум.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "        std_noise: float\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            states = torch.tensor(states, device=self.device)\n",
    "            # dist = Normal(0, std_noise)\n",
    "            actions = self.forward(states)\n",
    "            actions += torch.normal(0, std_noise, size=actions.shape, device=self.device)\n",
    "            actions = torch.clamp(actions, -1., 1.)\n",
    "\n",
    "            actions = actions.cpu().numpy()\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def get_target_action(self, states, std_noise=0.2, clip_eta=0.5):\n",
    "        '''\n",
    "        Используется для генерации целевых значений при обучении критика.\n",
    "        Возвращает действия с добавленным \"обрезанным шумом\".\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        std_noise: float\n",
    "        clip_eta: float\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: torch.tensor [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут также не нужны, используется только для генерации целевых значений\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            actions = self.forward(states)\n",
    "            raw_noise = Normal(0, std_noise).sample(actions.shape).to(self.device)\n",
    "            noise = torch.clamp(raw_noise, -clip_eta, clip_eta)\n",
    "            actions = actions + noise\n",
    "            actions = torch.clamp(actions, -1., 1.)\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.166671Z",
     "start_time": "2025-04-24T08:13:50.004540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:50.990771Z",
     "start_time": "2025-04-24T08:13:50.988283Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 10**6\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.367535Z",
     "start_time": "2025-04-24T08:13:51.358427Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы стабилизировать обучение, нам понадобятся целевые сети &mdash; медленно обновляемые копии наших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:51.835845Z",
     "start_time": "2025-04-24T08:13:51.822962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_actor = TD3_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах с непрерывным управлением целевые сети обычно обновляются с использованием экспоненциального сглаживания:\n",
    "$$\\theta^{-} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^{-},$$\n",
    "где $\\theta^{-}$ &mdash; веса дополнительной сети, $\\theta$ &mdash; текущие параметры модели, а $\\tau$ &mdash; гиперпараметр (коэффициент обновления)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.256363Z",
     "start_time": "2025-04-24T08:13:52.250709Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_target_networks(model, target_model, tau=0.005):\n",
    "    for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, у нас будет три отдельных процедуры оптимизации для обучения трёх моделей, так что давайте поприветствуем наших трёх Адамов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:52.658128Z",
     "start_time": "2025-04-24T08:13:52.650846Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:53.063674Z",
     "start_time": "2025-04-24T08:13:53.057164Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(name, model, optimizer, loss, n_iterations, max_grad_norm=10):\n",
    "    '''\n",
    "    Выполняет один шаг оптимизации, ограничивает норму градиента значением max_grad_norm\n",
    "    и логирует всё в TensorBoard.\n",
    "    '''\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    env.writer.add_scalar(name, loss.item(), n_iterations)\n",
    "    env.writer.add_scalar(name + \"_grad_norm\", grad_norm.item(), n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для одного сэмплированного перехода $(s, a, r, s')$ целевое значение имеет вид:\n",
    "$$y(s, a) = r + \\gamma V(s').$$\n",
    "Как вычисляется $V(s')$? Формально, оценка Монте-Карло выглядит просто:\n",
    "$$V(s') \\approx Q(s', a'),$$\n",
    "где $a'$ &mdash; это сэмпл из текущей политики $\\pi(a' \\mid s')$.\n",
    "\n",
    "Однако наш актор $\\pi$ обучается выбирать такие действия $a'$, где критик выдаёт наибольшие значения, что может приводить к завышенным оценкам. Чтобы избежать этого, мы используем несколько приёмов:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1,2} \\left\\{Q^{-}_i(s', a')\\right\\},$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики: $\\pi^{-}(a' \\mid s')$.\n",
    "2. Для вычисления $a'$ используется режим с обрезанным шумом, чтобы не позволить политике эксплуатировать узкие пики в $Q$-функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.138205Z",
     "start_time": "2025-04-24T01:58:06.138199Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.158713Z",
     "start_time": "2025-04-24T08:13:54.154703Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### Ваш код\n",
    "        is_not_done = 1 - is_done\n",
    "        next_actions = target_actor.get_target_action(next_states)#std_noise=0.05, clip_eta=0.2)\n",
    "        next_actions = torch.tensor(next_actions, device=DEVICE)\n",
    "        q1 =  target_critic1.get_qvalues(next_states, next_actions)\n",
    "        q2 =  target_critic2.get_qvalues(next_states, next_actions)\n",
    "        critic_target = rewards + gamma * torch.min(q1, q2) * is_not_done\n",
    "        ###\n",
    "\n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения актора мы хотим просто максимизировать:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} \\left[Q(s, a)\\right] \\to \\max_{\\pi}.$$\n",
    "Так как политика детерминированная, математическое ожидание сводится к:\n",
    "$$Q(s, \\pi(s)) \\to \\max_{\\pi}.$$\n",
    "\n",
    "**Замечание:**  \n",
    "Мы будем использовать `critic1` в качестве функции $Q$, которую актор будет пытаться \"эксплуатировать\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:54.997518Z",
     "start_time": "2025-04-24T08:13:54.991477Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states, actor, critic1):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "    ### Ваш код\n",
    "    actions = actor.get_best_action(states)\n",
    "    q = critic1.get_qvalues(states, actions)\n",
    "    actor_loss = -q.mean()\n",
    "    ###\n",
    "\n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **300 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:55.748252Z",
     "start_time": "2025-04-24T08:13:55.741014Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 4242 # Иногда может сильно не повезти\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:13:57.684751Z",
     "start_time": "2025-04-24T08:13:56.642708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-88e61a31147b3557\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-88e61a31147b3557\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:14:02.418310Z",
     "start_time": "2025-04-24T08:14:02.404628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=1500000, timesteps_per_epoch=1, start_timesteps = 5000,\n",
    "            batch_size=128, policy_update_freq=2, gamma=0.99, tau=0.005):\n",
    "\n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    ### Мой код\n",
    "    mse = nn.MSELoss()\n",
    "    ###\n",
    "\n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "\n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        _, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_computed = compute_critic_target(rewards, next_states, is_done, target_actor, target_critic1, target_critic2, gamma)\n",
    "\n",
    "        q_crititc1 = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = mse(q_crititc1, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_crititc2 = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = mse(q_crititc2, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            ### Ваш код\n",
    "            actor_loss = compute_actor_loss(states, actor, critic1)\n",
    "            ###\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)\n",
    "            update_target_networks(actor, target_actor, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:15.155234Z",
     "start_time": "2025-04-24T08:14:03.190411Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TD3_Actor:\n\tMissing key(s) in state_dict: \"net.0.weight\", \"net.0.bias\", \"net.2.weight\", \"net.2.bias\", \"net.4.weight\", \"net.4.bias\". \n\tUnexpected key(s) in state_dict: \"base.0.weight\", \"base.0.bias\", \"base.2.weight\", \"base.2.bias\", \"mean_head.weight\", \"mean_head.bias\", \"std_head.weight\", \"std_head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights/TD3/best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./weights/TD3/best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2,\n\u001b[1;32m      7\u001b[0m             max_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_iter_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m, timesteps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25_000\u001b[39m,\n\u001b[1;32m      8\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, policy_update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n",
      "File \u001b[0;32m~/Experiments/ReinforcedLearning/Непрерывное_управление/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2589\u001b[0m             ),\n\u001b[1;32m   2590\u001b[0m         )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2596\u001b[0m         )\n\u001b[1;32m   2597\u001b[0m     )\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TD3_Actor:\n\tMissing key(s) in state_dict: \"net.0.weight\", \"net.0.bias\", \"net.2.weight\", \"net.2.bias\", \"net.4.weight\", \"net.4.bias\". \n\tUnexpected key(s) in state_dict: \"base.0.weight\", \"base.0.bias\", \"base.2.weight\", \"base.2.bias\", \"mean_head.weight\", \"mean_head.bias\", \"std_head.weight\", \"std_head.bias\". "
     ]
    }
   ],
   "source": [
    "run_TD3(env, exp_replay, actor, target_actor, critic1, target_critic1, critic2, target_critic2,\n",
    "        max_grad_norm=10, n_iter_max=1_000_000, timesteps_per_epoch=1, start_timesteps=25_000,\n",
    "        batch_size=256, policy_update_freq=2, gamma=0.99, tau=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.141979Z",
     "start_time": "2025-04-24T01:58:06.141974Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:19.633149Z",
     "start_time": "2025-04-24T08:42:17.757596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша награда: 305.8536376953125\n",
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша награда: {score}\")\n",
    "\n",
    "try:\n",
    "    assert score >= 300, \"Нужно больше учить?\"\n",
    "    print(\"Отлично!\")\n",
    "except:\n",
    "    print(\"не получилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(actor.state_dict(), f'weights/TD3/{score}_{datetime.datetime.now()}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.976927Z",
     "start_time": "2025-04-24T08:42:22.393814Z"
    }
   },
   "outputs": [],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_TD3\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:42:23.980665Z",
     "start_time": "2025-04-24T08:42:23.978191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_TD3/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_TD3').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:31.821856Z",
     "start_time": "2025-04-24T08:59:31.818368Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"SAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно смоделировать гауссовскую политику. Это означает, что распределение политики &mdash; это многомерное нормальное распределение с диагональной ковариационной матрицей. Должны предсказываться среднее и ковариация, при этом важно гарантировать, что ковариация остаётся неотрицательной. Пусть $f_{\\theta}(s)$ &mdash; выход головы ковариации, преобразуем выход в диапазон $[-1, 1]$ с помощью `tanh`, затем спроецируем результат в интервал $[m, M]$, где $m = -20$, $M = 2$, и применим экспоненту. Это обеспечит адекватный диапазон ковариации:\n",
    "$$\\sigma(s) = \\exp^{m + 0.5(M - m)(\\tanh(f_{\\theta}(s)) + 1)}.$$\n",
    "\n",
    "Гауссовское распределение не ограничено, но нужно, чтобы действия лежали в диапазоне $[-1, 1]$. Для этого:\n",
    "1. Моделируем неограниченное распределение:  \n",
    "   $\\mathcal{N}(\\mu_{\\theta}(s), \\sigma_{\\theta}(s)^2I)$\n",
    "2. Затем каждую выборку $u$ из этого распределения преобразуем с помощью $\\tanh$:\n",
    "$$u \\sim \\mathcal{N}(\\mu, \\sigma^2I)$$\n",
    "$$a = \\tanh(u)$$\n",
    "\n",
    "**Важно:** После применения $\\tanh$ необходимо использовать формулу замены переменных при вычислении логарифма плотности вероятности:\n",
    "$$\\log p(a \\mid \\mu, \\sigma) = \\log p(u \\mid \\mu, \\sigma) - \\sum_{i = 1}^D \\log \\left(1 - \\tanh^2(u_i)\\right),$$\n",
    "где $D$ &mdash; размерность действия (`action_dim`). На практике желательно добавить небольшое значение (например, $1e{-6}$) внутрь логарифма для числовой устойчивости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(2 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:36.275645Z",
     "start_time": "2025-04-24T08:59:36.272221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAC_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, device, hidden_dim=256, m=-20, M=2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.m = m\n",
    "        self.M = M\n",
    "\n",
    "        ### Ваш код\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.std_head = nn.Linear(hidden_dim, action_dim)\n",
    "        ###\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        ### Ваш код\n",
    "        x = self.base(state)\n",
    "        mean = self.mean_head(x)\n",
    "\n",
    "        raw_std = self.std_head(x)\n",
    "        scale = torch.tanh(raw_std)\n",
    "        scale = self.m + 0.5 * (self.M - self.m) * (scale + 1)\n",
    "        cov = torch.exp(scale)\n",
    "        ###\n",
    "\n",
    "        return mean, cov\n",
    "\n",
    "    def sample(self, states):\n",
    "        '''\n",
    "        Сэмплирует действия.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "            states: torch.tensor [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "            actions: torch.tensor [batch_size x actions_dim]\n",
    "            log_prob: torch.tensor [batch_size]\n",
    "        '''\n",
    "        ### Ваш код\n",
    "        mean, std = self.forward(states)\n",
    "        dist = Normal(mean, std)\n",
    "        u = dist.rsample()\n",
    "        actions = torch.tanh(u)\n",
    "        log_prob = dist.log_prob(u)\n",
    "        log_prob -= ((1 - actions.pow(2) + 1e-6).log())\n",
    "        log_prob = log_prob.sum(dim=-1)\n",
    "        ###\n",
    "\n",
    "        return actions, log_prob\n",
    "\n",
    "    def get_action(self, states):\n",
    "        '''\n",
    "        Используется для взаимодействия с окружающей средой и сбора данных.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        states: np.array [batch_size x features]\n",
    "\n",
    "        Возвращает\n",
    "        -------\n",
    "        actions: np.array [batch_size x actions_dim]\n",
    "        '''\n",
    "        # Градиенты тут не нужны, так как используется только для взаимодействия\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ### Ваш код\n",
    "            states = torch.tensor(states, device=self.device, dtype=torch.float32)\n",
    "            actions, _ = self.sample(states)\n",
    "            actions = actions.cpu().numpy()\n",
    "            ###\n",
    "\n",
    "            assert actions.max() <= 1. and actions.min() >= -1,\\\n",
    "                \"Действия должны находиться в диапазоне [-1, 1]\"\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:39.079174Z",
     "start_time": "2025-04-24T08:59:38.889928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record должен был добавить ровно 1000 шагов, но вместо этого добавил %i\" % len(exp_replay)\n",
    "\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample_random(10)\n",
    "\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,), \\\n",
    "        \"Батчи наблюдений и следующих состояний должны иметь форму (10, %d)\" % state_dim\n",
    "\n",
    "    assert act_batch.shape == (10, action_dim), \\\n",
    "        \"Батч действий должен иметь форму (10, 8), но вместо этого: %s\" % str(act_batch.shape)\n",
    "\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"Батч наград должен иметь форму (10,), но вместо этого: %s\" % str(reward_batch.shape)\n",
    "\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"Батч is_done должен иметь форму (10,), но вместо этого: %s\" % str(is_done_batch.shape)\n",
    "\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done должен быть строго True или False (или 1/0)\"\n",
    "\n",
    "print(\"Отлично!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.091591Z",
     "start_time": "2025-04-24T08:59:41.089590Z"
    }
   },
   "outputs": [],
   "source": [
    "max_buffer_size = 10**6\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:41.582761Z",
     "start_time": "2025-04-24T08:59:41.579812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim, DEVICE).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:42.130567Z",
     "start_time": "2025-04-24T08:59:42.127553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:50.695410Z",
     "start_time": "2025-04-24T08:59:50.686991Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление целевого значения для критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "1. Используем двух критиков (берём минимум из их значений):\n",
    "$$V(s') = \\min_{i = 1, 2} \\left\\{Q^{-}_i(s', a')\\right\\},$$\n",
    "где $a'$ &mdash; это сэмпл из целевой политики $\\pi(a' \\mid s')$;\n",
    "2. Добавляется энтропийный бонус:\n",
    "$$V(s') = \\min_{i = 1, 2} \\left\\{Q^{-}_i(s', a')\\right\\} - \\alpha \\log \\pi(a' \\mid s').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.145091Z",
     "start_time": "2025-04-24T01:58:06.145086Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:59:58.722072Z",
     "start_time": "2025-04-24T08:59:58.712927Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done, actor, target_critic1, target_critic2,\n",
    "                          gamma=0.99, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для критика.\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        rewards: torch.tensor [batch_size]\n",
    "        next_states: torch.tensor [batch_size x features]\n",
    "        is_done: torch.tensor [batch_size]\n",
    "        gamma: float\n",
    "        alpha: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        critic_target: torch.tensor [batch_size]\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ### Ваш код\n",
    "        is_not_done = 1 - is_done\n",
    "        next_actions, log_prob = actor.sample(next_states)\n",
    "        q1 = target_critic1.get_qvalues(next_states, next_actions)\n",
    "        q2 = target_critic2.get_qvalues(next_states, next_actions)\n",
    "        min_q = torch.min(q1, q2)\n",
    "        V = min_q - alpha * log_prob\n",
    "        critic_target = rewards + gamma * V * is_not_done\n",
    "        ###\n",
    "\n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия от **TD3**:\n",
    "\n",
    "Добавляется регуляризатор энтропии, чтобы стимулировать стохастичность политики:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} \\left[Q(s, a) - \\alpha \\log \\pi(a \\mid s)\\right] \\to \\max_{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(0.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:01.896966Z",
     "start_time": "2025-04-24T09:00:01.892117Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states, actor, critic1, alpha=0.4):\n",
    "    '''\n",
    "    Подсчет loss для актора.\n",
    "\n",
    "\n",
    "    Параметры\n",
    "        ----------\n",
    "        states: torch.tensor [batch_size x features]\n",
    "        alpha: float\n",
    "\n",
    "    Возвращает\n",
    "        -------\n",
    "        actor_loss: torch.tensor [batch_size]\n",
    "    '''\n",
    "    ### Ваш код\n",
    "    actions, log_prob = actor.sample(states)\n",
    "    q = critic1.get_qvalues(states, actions)\n",
    "    actor_loss = (-q + alpha * log_prob).mean()\n",
    "    ###\n",
    "\n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель &mdash; достичь в среднем хотя бы **250 награды**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:00:10.972995Z",
     "start_time": "2025-04-24T09:00:10.965573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x124535310>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Иногда может сильно не повезти\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:45:08.879108Z",
     "start_time": "2025-04-24T10:45:05.325186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-67f1f6071ec8e2b5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-67f1f6071ec8e2b5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:28:37.626011Z",
     "start_time": "2025-04-24T10:28:37.621755Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2,\n",
    "            max_grad_norm=10, n_iter_max=1000000, timesteps_per_epoch=1, start_timesteps = 5000,\n",
    "            batch_size=128, policy_update_freq=2, gamma=0.99, tau=0.005, alpha=0.1):\n",
    "\n",
    "    interaction_state, _ = env.reset()\n",
    "    random_actor = RandomActor()\n",
    "    ### Мой код\n",
    "    mse = nn.MSELoss()\n",
    "    ###\n",
    "\n",
    "    for n_iterations in trange(0, n_iter_max, timesteps_per_epoch):\n",
    "\n",
    "        if len(exp_replay) < start_timesteps:\n",
    "            _, interaction_state = play_and_record(interaction_state, random_actor, env,\n",
    "                                                   exp_replay, timesteps_per_epoch)\n",
    "            continue\n",
    "\n",
    "        _, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        states, actions, rewards, next_states, is_done = exp_replay.sample_random(batch_size)\n",
    "\n",
    "        states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "        is_done = torch.tensor(is_done.astype('float32'), device=DEVICE, dtype=torch.float)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_computed = compute_critic_target(rewards, next_states, is_done, actor, target_critic1, target_critic2, gamma, alpha)\n",
    "\n",
    "        q_crititc1 = critic1.get_qvalues(states, actions)\n",
    "        critic1_loss = mse(q_crititc1, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic1\", critic1, opt_critic1, critic1_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        ### Ваш код\n",
    "        q_crititc2 = critic2.get_qvalues(states, actions)\n",
    "        critic2_loss = mse(q_crititc2, q_computed)\n",
    "        ###\n",
    "        optimize(\"critic2\", critic2, opt_critic2, critic2_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "        if n_iterations % policy_update_freq == 0:\n",
    "            ### Ваш код\n",
    "            actor_loss = compute_actor_loss(states, actor, critic1, alpha)\n",
    "            ###\n",
    "            optimize(\"actor\", actor, opt_actor, actor_loss, n_iterations, max_grad_norm)\n",
    "\n",
    "            update_target_networks(critic1, target_critic1, tau)\n",
    "            update_target_networks(critic2, target_critic2, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:44:15.948407Z",
     "start_time": "2025-04-24T10:28:38.990752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f940b3cf094c00927c63fdcc25343a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_replay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critic2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_update_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 20\u001b[0m, in \u001b[0;36mrun_SAC\u001b[0;34m(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2, max_grad_norm, n_iter_max, timesteps_per_epoch, start_timesteps, batch_size, policy_update_freq, gamma, tau, alpha)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     18\u001b[0m _, interaction_state \u001b[38;5;241m=\u001b[39m play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n\u001b[0;32m---> 20\u001b[0m states, actions, rewards, next_states, is_done \u001b[38;5;241m=\u001b[39m \u001b[43mexp_replay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(states, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     23\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mReplayBuffer.sample_random\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m---> 45\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     46\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(sample[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(sample[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_SAC(env, exp_replay, actor, critic1, target_critic1, critic2, target_critic2,\n",
    "        max_grad_norm=10, n_iter_max=2_000_000, timesteps_per_epoch=1, start_timesteps=25_000,\n",
    "        batch_size=256, policy_update_freq=1, gamma=0.99, tau=0.005, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T01:58:06.147830Z",
     "start_time": "2025-04-24T01:58:06.147825Z"
    }
   },
   "source": [
    "<span style=\"color: green\"> __(1.5 балла)__ </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:46:58.036798Z",
     "start_time": "2025-04-24T10:46:52.730916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша награда: -78.97158813476562\n",
      "не получилось\n"
     ]
    }
   ],
   "source": [
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Ваша награда: {score}\")\n",
    "\n",
    "try:\n",
    "    assert score >= 250, \"Нужно больше учить?\"\n",
    "    print(\"Отлично!\")\n",
    "except:\n",
    "    print(\"не получилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), f'weights/SAC/{score}_{datetime.datetime.now()}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.332491Z",
     "start_time": "2025-04-24T10:27:43.771771Z"
    }
   },
   "outputs": [],
   "source": [
    "with RecordVideo(\n",
    "    env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos_SAC\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:27:45.336278Z",
     "start_time": "2025-04-24T10:27:45.333699Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos_SAC/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos_SAC').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: green\"> __(5 баллов)__ </span> Выбить хотя бы **300 награды** в режиме хардкора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"Hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ваше решение"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
